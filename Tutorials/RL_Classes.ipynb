{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the value of selecting an action as the expected reward we receive when taking bad action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q^*(a) \\doteq \\mathbb{E}[R_t | A_t = a] \\quad \\forall a \\in \\{1, \\dots, k\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation above explains that the conditional expectation is defined as the expectation of reward ($R_t$) given we selectd the action $a$. The goal is to **maximize** the **expected reward**. It is important to highight that the $q^*(a)$ value is not known, so we estimate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\argmax_a{q^*(a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On way to estimated this value is through the *Sample-Average Method*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_t(a) = \\frac{\\text{sum of rewards when } a \\text{ taken prior to } t }{\\text{number of times } a \\text{ taken prior to } t }\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "Q_t(a) = \\frac{\\sum^{t-1}_{i=1}{R_i}}{t-1} \\implies Q_{t+1}(a) = \\frac{\\sum^{t}_{i=1}{R_i}}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will determine the **greedy action** to the problem. The greedy action is the action that currently has the largest estimated value. Selecting the greedy action means the agent is exploiting its current knowledge. It means that the agent is trying to get the most reward it can. It is possible to write the estimated value by the incremental update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{n+1} = Q_n + a_n (R_n - Q_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the estimate is the difference between the old estimate and the new target. Taking a step towards that new target will create a new estimate that reduces our error. The step size can be a function of $n$ that produces a number from zero to one ($a_n \\to [0,1]$ or $a_n = \\frac{1}{n}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration and Exploation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the difference between explorarion and exploitation will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explorarion:** The exploration improve knowledge for *long-term* benefit. Here you will explore all the oportunities, aiming to find the best solution to your problem.\n",
    "- **Exploitation:** The exploitation exploit knowledge for *shot-term* benefit. Here the other opportunities will not be explored and the best solution won't be discovered.\n",
    "- **The Dilemma:** How do we choose when to explore and when to exploit?\n",
    "    - Use the *epsilon-greedy* method to balance the explorarion and exploitation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a state $S$ and action $a$, $p$ tells us the joint probability of next state $s'$ and reward $r$ are. Since $p$ is a probability distribution, it must be non-negative and it's sum over all possible next states and rewards must equal one. Note that future state and reward only depends on the current state and action. This is called the Markov property. It means that the present state is sufficient and remembering earlier states would not improve predictions about the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(s', r | s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state value function is the future award an agent can expect to receive starting from a particular state. More precisely, the state value function is the expected return from a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) \\doteq \\mathbb{E_\\pi}[G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An action value describes what happens when the agent first selects a particular action. More formally, the action value of a state is the expected return if the agent selects action $a$ and then follows the policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_\\pi(s,a) \\doteq \\mathbb{E_\\pi}[G_t | S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fb91b1e956d45f6f365b9843a173f2d748faeda26451ace645ced11690f22b1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
